{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4cc66715-74b7-453d-a3c2-b4c8647cac62",
      "cell_type": "code",
      "source": "# Meme classification: Multimodal hate speech detection\n* Venkata Koushik Nagasarapu",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ebee72e1-15f7-4957-80d8-a0903c0e028b",
      "cell_type": "code",
      "source": "Motivation: I chose this paper because it addresses a pressing real-world problem—hate speech detection in memes—while highlighting \nthe unique challenges of truly multimodal reasoning. Memes combine text and imagery in subtle, often sarcastic ways, making unimodal \napproaches inadequate. This aligns perfectly with DA623’s focus on multimodal data analysis.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0ffa3f5-9904-42ce-a500-d72a728fa0b8",
      "cell_type": "code",
      "source": "Historical Perspective in Multimodal Learning:\n1. Early Vision + Language Tasks : Image captioning (e.g.MS COCO Captions) and Visual Question Answering (VQA) showed strong unimodal biases.\n2. Contrastive & Counterfactual Data : Subsequent work (e.g. CRIC, CLEVR) introduced contrast sets to force true multimodal understanding.\n3. Hateful Memes Challenge : Builds on this by embedding “benign confounders” so that text or image alone is insufficient.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "274ded78-38a6-4801-b7b9-d57ed4a27964",
      "cell_type": "code",
      "source": "A. Dataset Construction:\n  1. Extract ~162k memes, filter for English/non-violating, license Getty replacements, reconstruct PNG/SVG.\n  2. Four annotation phases: Filtering → Reconstruction → Hatefulness Rating → Benign Confounder Creation.\nB. Hatefulness Definition: “A direct or indirect attack on people based on protected characteristics (race, religion, gender, etc.)…”\nC. Benign Confounders: For each hateful meme, generate minimal text/image swaps that flip label → forces multimodal fusion.\nD. Data Splits: Total 10k memes → 5% dev, 10% test (balanced: 40% multimodal hate, 10% unimodal hate, 20% each benign confounder,\n   10% non-hateful).\nE. Analysis Highlights:\n    1. Moderate inter-annotator κ = 0.684.\n    2. Protected categories: Race (47.1%), Religion (39.3%), Gender (14.8%), etc.\n    3. Attack types: e.g. comparison to criminals (17.2%), negative stereotypes (15.6%) \nF. Baseline Results:\n    | Model                | Test AUROC |\n    | -------------------- | ---------  |\n    | Human                |   — / 84.7 |\n    | Text-only (BERT)     |       69.0 |\n    | Vision-only (ResNet) |       53.7 |\n    | Late Fusion          |       69.3 |\n    | MMBT-Region          |       73.8 |\n    | ViLBERT CC           |       74.5 |\n    | VisualBERT COCO      |       75.4 |\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "45698d01-f488-494b-8f7d-5d25cb4e230d",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom IPython.display import display\nimport os\n\n# Load dataset (JSONL format)\ndf = pd.read_json('hateful_memes_dev.jsonl', lines=True)\nprint(\"Dataset shape:\", df.shape)\ndf.head()\n\n# Map label to text\ndf['label_text'] = df['label'].map({0: 'Non-Hateful', 1: 'Hateful'})\n\n# === 1. Show example hateful meme + confounder ===\n\nsample_hateful = df[df['label'] == 1].iloc[0]\nprint(\"Original hateful meme text:\", sample_hateful['text'])\nimg = Image.open(sample_hateful['img'])\ndisplay(img)\n\n# === 2. Text Length Distribution ===\ndf['text_length'] = df['text'].apply(lambda x: len(x.split()))\nsns.histplot(data=df, x='text_length', hue='label_text', bins=15)\nplt.title('Text Length Distribution')\nplt.show()\n\n# === 3. Word Frequencies ===\nfrom collections import Counter\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\ndef get_word_counts(texts):\n    all_words = ' '.join(texts).lower().split()\n    words = [w for w in all_words if w.isalpha() and w not in stop_words]\n    return Counter(words)\n\nhateful_counts = get_word_counts(df[df['label']==1]['text'])\nnon_hateful_counts = get_word_counts(df[df['label']==0]['text'])\n\n# Plot top 10 words\nhateful_df = pd.DataFrame(hateful_counts.most_common(10), columns=['word', 'count'])\nnon_hateful_df = pd.DataFrame(non_hateful_counts.most_common(10), columns=['word', 'count'])\n\nfig, axs = plt.subplots(1, 2, figsize=(14,5))\nsns.barplot(data=hateful_df, x='count', y='word', ax=axs[0], palette='Reds_r')\naxs[0].set_title('Top Hateful Words')\nsns.barplot(data=non_hateful_df, x='count', y='word', ax=axs[1], palette='Blues_r')\naxs[1].set_title('Top Non-Hateful Words')\nplt.show()\n\n# === 4. Class Distribution ===\nsns.countplot(data=df, x='label_text', palette='pastel')\nplt.title('Class Distribution')\nplt.show()\n\n# === 5. Simple Text-Only Baseline (TF-IDF + Logistic Regression) ===\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# TF-IDF features\nvectorizer = TfidfVectorizer(max_features=1000)\nX = vectorizer.fit_transform(df['text'])\ny = df['label']\n\n# Logistic Regression\nmodel = LogisticRegression()\nscores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\nprint(\"Text-Only Baseline AUROC: {:.2f} (+/- {:.2f})\".format(scores.mean()*100, scores.std()*100))\n\n# === 6. Simple Vision-Only Baseline (ImageNet features + Logistic Regression) ===\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\n\n# Pre-trained ResNet50\nresnet = models.resnet50(pretrained=True)\nresnet.eval()\n\n# Remove last layer to get 2048-D features\nfeature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ndef extract_features(img_path):\n    img = Image.open(img_path).convert('RGB')\n    img_t = transform(img).unsqueeze(0)  # batch of 1\n    with torch.no_grad():\n        feat = feature_extractor(img_t).squeeze().numpy()\n    return feat\n\n# Extract features for all images\nimg_feats = []\nfor path in tqdm(df['img'].tolist(), desc=\"Extracting image features\"):\n    img_feats.append(extract_features(path))\n\nX_img = np.array(img_feats)\n\n# Logistic Regression\nscores_img = cross_val_score(LogisticRegression(max_iter=1000), X_img, y, cv=5, scoring='roc_auc')\nprint(\"Image-Only Baseline AUROC: {:.2f} (+/- {:.2f})\".format(scores_img.mean()*100, scores_img.std()*100))\n\n# === 7. Optional: Fusion Baseline (Concatenate Text+Image) ===\nfrom scipy import sparse\n\nX_fusion = sparse.hstack([X, sparse.csr_matrix(X_img)])\nscores_fusion = cross_val_score(LogisticRegression(max_iter=1000), X_fusion, y, cv=5, scoring='roc_auc')\nprint(\"Fusion Baseline AUROC: {:.2f} (+/- {:.2f})\".format(scores_fusion.mean()*100, scores_fusion.std()*100))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bf1b705c-047c-430b-b0ca-be8258569d9d",
      "cell_type": "code",
      "source": "Reflections:\nA. Surprises: \n  1.Even strong multimodal models fail to bridge the gap to humans.\n  2.The benign confounder strategy effectively neutralizes unimodal shortcuts.\nB. Scope for Improvement:\n  1.Better multimodal pretraining (e.g. larger contrastive objectives).\n  2.Incorporating world knowledge (e.g. geopolitics, historical events).\n  3.Dynamic data augmentations to cover evolving hate symbols.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2e1e9534-c744-4b86-9c45-a244198f6a89",
      "cell_type": "code",
      "source": "References:\n1. Kiela, D., Firooz, H., et al. The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes. arXiv:2005.04790 (2021).\n2. Facebook AI. MMF code & starter kit: https://github.com/facebookresearch/mmf/projects/hateful_memes",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}